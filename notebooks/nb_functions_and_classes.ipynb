{"cells":[{"cell_type":"code","source":["from dataclasses import dataclass\n","from pyspark.sql import DataFrame\n","from pyspark.sql.functions import explode, lit, col\n","from delta.tables import DeltaTable\n","from datetime import datetime\n","import re\n","\n","\n","def str_to_snake_case(name: str) -> str:\n","    return re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower()\n","\n","def snake_case_columns(data: any) -> any:\n","    if isinstance(data, DataFrame):\n","        return data.select([data[col].alias(str_to_snake_case(col)) for col in data.columns])\n","    elif isinstance(data, list):\n","        return [str_to_snake_case(col) for col in data]\n","    else:\n","        raise TypeError(\"Input should be a DataFrame or a list of column names\")\n","\n","def table_exists(table_name: str) -> bool:\n","    try:\n","        return spark.catalog._jcatalog.tableExists(table_name)\n","    except Exception:\n","        return False\n","\n","\n","def explode_df_columns(df: DataFrame, column_to_explode: str) -> DataFrame:\n","    exploded_df = df.withColumn(column_to_explode,\n","                                explode(col(column_to_explode)))\n","    fields = exploded_df.schema[column_to_explode].dataType.fields\n","    select_exprs = [\n","        col(f\"{column_to_explode}.{field.name}\").alias(field.name)\n","        for field in fields\n","    ]\n","    return exploded_df.select(*select_exprs)\n","\n","\n","@dataclass\n","class MetadataFetcher:\n","    entity_id: int\n","\n","    def fetch(self,\n","              table,\n","              value_column,\n","              filter_column=None,\n","              filter_value=None,\n","              parameter_name=None) -> str:\n","        query = f\"SELECT {value_column} FROM lh_metadata.{table}\"\n","        if filter_column and filter_value:\n","            query += f\" WHERE {filter_column} = {filter_value}\"\n","        if parameter_name:\n","            query += f\" AND parameter_name = '{parameter_name}'\"\n","        try:\n","            df = spark.sql(query)\n","            result = df.collect()\n","            if result:\n","                return result[0][0]\n","            else:\n","                raise ValueError(f\"No value found for:\\n\"\n","                                 f\"  table: {table}\\n\"\n","                                 f\"  value_column: {value_column}\\n\"\n","                                 f\"  filter_column: {filter_column}\\n\"\n","                                 f\"  filter_value: {filter_value}\\n\"\n","                                 f\"  parameter_name: {parameter_name}\")\n","        except Exception as e:\n","            print(f\"Error executing query: {e}\")\n","            raise\n","\n","    def fetch_file_format(self) -> str:\n","        try:\n","            result = self.fetch(\"entity_parameters\", \"parameter_value\",\n","                                \"entity_id\", self.entity_id,\n","                                \"sink_file_format\")\n","            return result\n","        except Exception as e:\n","            print(f\"Failed to fetch file format: {e}\")\n","            raise\n","\n","    def fetch_column_to_explode(self) -> str:\n","        try:\n","            result = self.fetch(\"entity_parameters\", \"parameter_value\",\n","                                \"entity_id\", self.entity_id,\n","                                \"column_to_explode\")\n","            return result\n","        except Exception as e:\n","            print(f\"Failed to fetch column to explode: {e}\")\n","            raise\n","\n","    def fetch_entity_name(self) -> str:\n","        try:\n","            result = self.fetch(\"entity\", \"name\", \"id\", self.entity_id)\n","            return result\n","        except Exception as e:\n","            print(f\"Failed to fetch entity name: {e}\")\n","            raise\n","\n","    def fetch_source_id(self) -> int:\n","        try:\n","            result = self.fetch(\"entity\", \"source_id\", \"id\", self.entity_id)\n","            return result\n","        except Exception as e:\n","            print(f\"Failed to fetch source ID: {e}\")\n","            raise\n","\n","    def fetch_source_name(self) -> str:\n","        try:\n","            source_id = self.fetch_source_id()\n","            result = self.fetch(\"source\", \"name\", \"id\", source_id)\n","            return result\n","        except Exception as e:\n","            print(f\"Failed to fetch source name: {e}\")\n","            raise\n","\n","    def fetch_processing_type(self) -> str:\n","        try:\n","            result = self.fetch(\"entity\", \"processing_type\", \"id\",\n","                                self.entity_id)\n","            return result\n","        except Exception as e:\n","            print(f\"Failed to fetch processing type: {e}\")\n","            raise\n","\n","    def fetch_key_columns(self) -> str:\n","        try:\n","            result = self.fetch(\"entity_parameters\", \"parameter_value\",\n","                                \"entity_id\", self.entity_id, \"key_columns\")\n","            return result\n","            print(result)\n","        except Exception as e:\n","            print(f\"Failed to fetch key columns: {e}\")\n","            raise\n","\n","\n","@dataclass\n","class FileProcessor(MetadataFetcher):\n","    file_path: str\n","\n","    def __post_init__(self):\n","        self.format_readers = {\n","            'json': spark.read.json,\n","            'parquet': spark.read.parquet\n","            # more formats if needed\n","        }\n","\n","    def insert_processing_log(self, layer: str, status: str):\n","        \"\"\"\n","        Inserts a processing log entry into the processing_logs table.\n","\n","        Args:\n","            layer (str): The processing layer ('bronze' or 'silver').\n","            status (str): The processing status.\n","        \n","        Raises:\n","            Exception: If an error occurs while writing the log entry.\n","        \"\"\"\n","        log_data = [(self.entity_id, layer, self.file_path, datetime.now(),\n","                     status)]\n","\n","        try:\n","            df = spark.createDataFrame(log_data,\n","                                       schema=[\n","                                           \"entity_id\", \"layer\", \"file_path\",\n","                                           \"processing_time\", \"status\"\n","                                       ])\n","            df = df.withColumn(\"entity_id\", col(\"entity_id\").cast(\"integer\"))\n","            df.write.mode(\"append\").saveAsTable(\"lh_logging.processing_logs\")\n","            print(\n","                \"Log entry successfully written to lh_logging.processing_logs\")\n","        except AnalysisException as ae:\n","            print(f\"AnalysisException: {ae}\")\n","        except Exception as e:\n","            print(f\"An error occurred while writing the log entry: {e}\")\n","\n","    def read_file_to_dataframe(self) -> DataFrame:\n","        \"\"\"\n","        Reads the file into a DataFrame. Supports JSON and Parquet formats.\n","\n","        Returns:\n","            DataFrame: The data read from the file.\n","        \n","        Raises:\n","            ValueError: If the file format is unsupported.\n","        \"\"\"\n","        file_format = self.fetch_file_format()\n","        if file_format in self.format_readers:\n","            df = self.format_readers[file_format](self.file_path)\n","            if file_format == \"json\":\n","                df = explode_df_columns(df, self.fetch_column_to_explode())\n","        else:\n","            raise ValueError(f\"Unsupported file format: {file_format}\")\n","        return df\n","\n","    def business_key_duplicates(self, df: DataFrame) -> bool:\n","        \"\"\"\n","        Checks for duplicates based on business key columns.\n","\n","        Args:\n","            df (DataFrame): The DataFrame to check for duplicates.\n","\n","        Returns:\n","            bool: True if duplicates are found, False otherwise.\n","        \"\"\"\n","        listed_key_columns = eval(self.fetch_key_columns())\n","        return df.select(*listed_key_columns).distinct().count() != df.count()\n","\n","    def _merge_to_table(self, df: DataFrame, table_name: str,\n","                        key_columns: list):\n","        \"\"\"\n","        Merges the DataFrame into the specified table using the key columns.\n","\n","        Args:\n","            df (DataFrame): The DataFrame to merge.\n","            table_name (str): The name of the table to merge into.\n","            key_columns (list): The key columns for merging.\n","\n","        Raises:\n","            Exception: If an error occurs during the merge.\n","        \"\"\"\n","        try:\n","            if table_exists(table_name):\n","                target_table = DeltaTable.forName(spark, table_name)\n","                merge_condition = \" AND \".join(\n","                    [f\"tgt.{col} = src.{col}\" for col in key_columns])\n","                target_table.alias(\"tgt\").merge(\n","                    df.alias(\"src\"), merge_condition).whenMatchedUpdateAll(\n","                    ).whenNotMatchedInsertAll().execute()\n","            else:\n","                df.write.saveAsTable(table_name)\n","        except Exception as e:\n","            raise Exception(f\"Error merging to table {table_name}: {e}\")\n","\n","    def write_file_to_bronze(self):\n","        \"\"\"\n","        Reads the file into a DataFrame, adds metadata columns, and writes it to the bronze table.\n","        \"\"\"\n","        try:\n","            df = self.read_file_to_dataframe()\n","            df = df.withColumn(\"_processed_datetime\", lit(datetime.now()))\n","            df = df.withColumn(\"_source_name\", lit(self.fetch_source_name()))\n","            df = df.withColumn(\"_file_path\", lit(self.file_path))\n","\n","            entity_name = self.fetch_entity_name()\n","\n","            # Write to bronze table\n","            df.write.mode(\"append\").saveAsTable(f\"lh_bronze.{entity_name}\")\n","\n","            self.insert_processing_log('bronze', 'success')\n","        except Exception as e:\n","            print(f\"Error writing to bronze: {e}\")\n","            self.insert_processing_log('bronze', f\"failure: {str(e)}\")\n","\n","    def write_file_to_silver(self):\n","        \"\"\"\n","        Reads the file into a DataFrame, applies quality assurance checks, and writes it to the silver table.\n","\n","        Raises:\n","            Exception: If an error occurs during processing or writing.\n","        \"\"\"\n","        try:\n","            df = self.read_file_to_dataframe()\n","            df = snake_case_columns(df).dropDuplicates()\n","\n","            if self.business_key_duplicates(df):\n","                raise ValueError(\"Business key duplicates found\")    \n","\n","            processing_type = self.fetch_processing_type()\n","            entity_name = self.fetch_entity_name()\n","            key_columns = eval(self.fetch_key_columns())\n","\n","            silver_table = f\"lh_silver.{entity_name}\"\n","            if processing_type == \"rebuild\":\n","                df.write.mode(\"overwrite\").saveAsTable(silver_table)\n","            elif processing_type == \"merge\":\n","                self._merge_to_table(df, silver_table, key_columns)\n","            else:\n","                raise ValueError(\n","                    f\"Unsupported processing type: {processing_type}\")\n","\n","            self.insert_processing_log(\"silver\", \"success\")\n","        except Exception as e:\n","            self.insert_processing_log(\"silver\", f\"failure: {str(e)}\")\n","            raise\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"livy_statement_state":"available","session_id":"f0c4d403-0739-428d-8f7b-24986363e442","state":"finished","normalized_state":"finished","queued_time":"2024-07-15T14:29:58.9626616Z","session_start_time":"2024-07-15T14:29:59.2564214Z","execution_start_time":"2024-07-15T14:30:07.9018788Z","execution_finish_time":"2024-07-15T14:30:10.6541464Z","parent_msg_id":"8131f596-06e3-4eff-a895-db4a451370c7"},"text/plain":"StatementMeta(, f0c4d403-0739-428d-8f7b-24986363e442, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9cb88c90-ff0c-4f91-94e4-cc5a8d1b4c0b"}],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"sessionKeepAliveTimeout":0,"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"a365ComputeOptions":null,"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"}},"nbformat":4,"nbformat_minor":5}