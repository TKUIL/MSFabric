{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n","\n","# Define schemas for the tables\n","source_schema = StructType([\n","    StructField(\"id\", IntegerType(), False),\n","    StructField(\"name\", StringType(), True),\n","    StructField(\"type\", StringType(), True),\n","    StructField(\"description\", StringType(), True)\n","])\n","entity_schema = StructType([\n","    StructField(\"id\", IntegerType(), False),\n","    StructField(\"source_id\", IntegerType(), False),\n","    StructField(\"name\", StringType(), True),\n","    StructField(\"processing_type\", StringType(), True),\n","    StructField(\"description\", StringType(), True)\n","])\n","entity_parameters_schema = StructType([\n","    StructField(\"id\", IntegerType(), False),\n","    StructField(\"entity_id\", IntegerType(), False),\n","    StructField(\"parameter_name\", StringType(), True),\n","    StructField(\"parameter_value\", StringType(), True),\n","    StructField(\"description\", StringType(), True)\n","])\n","\n","\n","# Helper function to retrieve ID value from DataFrame based on given name\n","def get_id_dict(df):\n","    return {\n","        row['name']: row['id']\n","        for row in df.select(\"id\", \"name\").collect()\n","    }\n","\n","\n","# Define source types\n","source_types = [\n","    (\"FakerAPI\", \"api\", \"API source for Faker data generation\"),\n","    (\"sql_f1_data\", \"sql\" , \"f1 data mart\")\n","    ]\n","\n","# Generate source data dynamically with enumerated IDs\n","source_data = [(i, source[0], source[1], source[2])\n","               for i, source in enumerate(source_types, start=1)]\n","df_source = spark.createDataFrame(source_data, schema=source_schema)\n","df_source.write.mode(\"overwrite\").saveAsTable(\"lh_metadata.source\")\n","\n","# Construct dictionary from the source table\n","source_id_dict = get_id_dict(df_source)\n","\n","\n","# Function to create entities dynamically with enumerated IDs\n","def create_entity_data(entities, source_id_dict):\n","    entity_data = []\n","    for i, entity in enumerate(entities, start=1):\n","        entity_dict = {\n","            'id': i,\n","            'source_id': source_id_dict[entity['source']],\n","            'name': entity['name'],\n","            'processing_type': entity['processing_type'],\n","            'description': f\"{entity['name']} from {entity['source']}\"\n","        }\n","        entity_data.append(\n","            (entity_dict['id'], entity_dict['source_id'], entity_dict['name'],\n","             entity_dict['processing_type'], entity_dict['description']))\n","    return entity_data\n","\n","\n","# Populate the entities list dynamically\n","names_merge = ['persons', 'products', 'users']\n","names_rebuild = ['addresses', 'companies', 'books']\n","names_merge_sql = ['fact_constructor_statistics', 'fact_driver_statistics']\n","names_rebuild_sql = ['dim_constructors', 'dim_drivers', 'dim_races', 'dim_status']\n","\n","# Dynamic generation of entities list with source field\n","entities = [{'source': 'FakerAPI', 'name': name, 'processing_type': 'merge', 'sink_file_folder': 'landing_zone', 'sink_file_format': 'json', 'api_query': f'{name}?_quantity=1000&_locale=en_EN'} for name in names_merge] + \\\n","           [{'source': 'FakerAPI', 'name': name, 'processing_type': 'rebuild', 'sink_file_folder': 'landing_zone', 'sink_file_format': 'json', 'api_query': f'{name}?_quantity=1000&_locale=en_EN'} for name in names_rebuild] + \\\n","           [{'source': 'sql_f1_data', 'name': name, 'processing_type': 'merge', 'sink_file_folder': 'landing_zone', 'sink_file_format': 'parquet'} for name in names_merge_sql] + \\\n","           [{'source': 'sql_f1_data', 'name': name, 'processing_type': 'rebuild', 'sink_file_folder': 'landing_zone', 'sink_file_format': 'parquet'} for name in names_rebuild_sql]\n","\n","# Generate entity data dynamically\n","entity_data = create_entity_data(entities, source_id_dict)\n","df_entity = spark.createDataFrame(entity_data, entity_schema)\n","df_entity.write.mode(\"overwrite\").saveAsTable(\"lh_metadata.entity\")\n","\n","\n","# Function to create entity parameters data dynamically with enumerated IDs\n","def create_entity_parameters_data(entities, df_entity):\n","    entity_parameters_data = []\n","    param_id = 1\n","\n","    # Local helper function to retrieve ID value from DataFrame based on given name\n","    def get_id(name, df):\n","        id_row = df.filter(df.name == name).select(\"id\").first()\n","        return id_row.id if id_row else None\n","\n","    for entity in entities:\n","        entity_id = get_id(entity['name'], df_entity)\n","        if entity['source'] == 'FakerAPI':\n","            parameters = [\n","                (entity_id, 'sink_file_format', entity['sink_file_format'], ''),\n","                (entity_id, 'sink_file_folder', entity['sink_file_folder'], ''),\n","                (entity_id, 'api_query', entity['api_query'], ''),\n","                (entity_id, 'column_to_explode', 'data', ''),\n","                (entity_id, 'key_columns', '[\"id\"]', '')\n","            ]\n","        else: # This is some stupid shit, would've been smarter to just fix the Data Mart objects\n","            name = entity[\"name\"]\n","            name = name.lower()\n","            if name == \"dim_status\":\n","                singular_name = \"dim_status\"\n","            elif name.startswith(\"dim_\"):\n","                singular_name = name[:-1] if name.endswith(\"s\") else name\n","            elif name.startswith(\"fact_\"):\n","                singular_name = name.replace(\"_statistics\", \"\")\n","            else:\n","                singular_name = name\n","\n","            key_column = f\"{singular_name}_key\"\n","\n","            # Format to add \"_key\" maintaining underscores\n","            key_column = f'[\"{singular_name}_key\"]'.lower()\n","            parameters = [\n","                (entity_id, 'sink_file_format', entity['sink_file_format'], ''),\n","                (entity_id, 'sink_file_folder', entity['sink_file_folder'], ''),\n","                (entity_id, 'key_columns', key_column, ''),\n","                (entity_id, 'source_schema', 't_dm', ''),\n","                (entity_id, 'target_schema', 'dbo', '')\n","            ]\n","        for param in parameters:\n","            entity_parameters_data.append((param_id, param[0], param[1], param[2], param[3]))\n","            param_id += 1\n","    return entity_parameters_data\n","\n","\n","# Generate entity parameters data dynamically\n","entity_parameters_data = create_entity_parameters_data(entities, df_entity)\n","df_entity_parameters = spark.createDataFrame(entity_parameters_data, entity_parameters_schema)\n","df_entity_parameters.write.mode(\"overwrite\").saveAsTable(\"lh_metadata.entity_parameters\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"15945498-9023-4aa2-bb29-33a416d725a1"},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n","spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n","spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n","spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")\n","\n","df_entity_parameters.show(truncate=False, n=1000)\n","df_source.show(truncate=False, n=1000)\n","df_entity.show(truncate=False, n=1000)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6a9b6267-9cf4-4caa-8fc1-8937f1fdc5f4"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"aa3d42e8-8e00-4a61-81fd-0127cd45c5c6","default_lakehouse_name":"lh_metadata","default_lakehouse_workspace_id":"7d0c7301-49ea-4c9a-96ea-c039dec314ba"}}},"nbformat":4,"nbformat_minor":5}